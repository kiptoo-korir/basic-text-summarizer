{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c1c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "011f7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file article\n",
    "def get_sentences(filetext):\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in filetext:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split())\n",
    "        \n",
    "    sentences.pop()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def get_sentence_from_file(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    filedata_combined = \"\".join([paragraph for paragraph in filedata])\n",
    "    filetext = filedata_combined.split(\". \")\n",
    "    sentences = get_sentences(filetext)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7eb7dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_from_dataset():\n",
    "    df = pd.read_csv('./bbc-news-data.csv')\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    random_no = random.randint(0, len(df) - 1)\n",
    "\n",
    "    # Get random article text from Data Set\n",
    "    article = df['content'][random_no].split(\". \")\n",
    "\n",
    "    sentences = get_sentences(article)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9341bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c2d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d521312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name='', top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text and tokenize\n",
    "    # If using text from a dataset, use method below and customize it to fit your dataset\n",
    "    # sentences =  get_sentence_from_dataset()\n",
    "    \n",
    "#     If using text from a .txt file, use method below\n",
    "    sentences = get_sentence_from_file(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afe90fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.14039469398100238, ['Dollar', 'gains', 'on', 'Greenspan', 'speech', 'The', 'dollar', 'has', 'hit', 'its', 'highest', 'level', 'against', 'the', 'euro', 'in', 'almost', 'three', 'months', 'after', 'the', 'Federal', 'Reserve', 'head', 'said', 'the', 'US', 'trade', 'deficit', 'is', 'set', 'to', 'stabilise.', 'And', 'Alan', 'Greenspan', 'highlighted', 'the', 'US', \"government's\", 'willingness', 'to', 'curb', 'spending', 'and', 'rising', 'household', 'savings', 'as', 'factors', 'which', 'may', 'help', 'to', 'reduce', 'it']), (0.13029461996080488, [\"China's\", 'currency', 'remains', 'pegged', 'to', 'the', 'dollar', 'and', 'the', 'US', \"currency's\", 'sharp', 'falls', 'in', 'recent', 'months', 'have', 'therefore', 'made', 'Chinese', 'export', 'prices', 'highly', 'competitive']), (0.10564213008326902, ['Market', 'concerns', 'about', 'the', 'deficit', 'has', 'hit', 'the', 'greenback', 'in', 'recent', 'months']), (0.09127819190589045, ['On', 'Friday,', 'Federal', 'Reserve', 'chairman', 'Mr', \"Greenspan's\", 'speech', 'in', 'London', 'ahead', 'of', 'the', 'meeting', 'of', 'G7', 'finance', 'ministers', 'sent', 'the', 'dollar', 'higher', 'after', 'it', 'had', 'earlier', 'tumbled', 'on', 'the', 'back', 'of', 'worse-than-expected', 'US', 'jobs', 'data']), (0.08549596830083447, ['The', 'recent', 'falls', 'have', 'partly', 'been', 'the', 'result', 'of', 'big', 'budget', 'deficits,', 'as', 'well', 'as', 'the', \"US's\", 'yawning', 'current', 'account', 'gap,', 'both', 'of', 'which', 'need', 'to', 'be', 'funded', 'by', 'the', 'buying', 'of', 'US', 'bonds', 'and', 'assets', 'by', 'foreign', 'firms', 'and', 'governments']), (0.07785104988372074, ['The', 'half-point', 'window,', 'some', 'believe,', 'could', 'be', 'enough', 'to', 'keep', 'US', 'assets', 'looking', 'more', 'attractive,', 'and', 'could', 'help', 'prop', 'up', 'the', 'dollar']), (0.07754514528367433, ['\"He\\'s', 'taking', 'a', 'longer-term', 'view,', 'laying', 'out', 'a', 'set', 'of', 'conditions', 'under', 'which', 'the', 'current', 'account', 'deficit', 'can', 'improve', 'this', 'year', 'and', 'next.\"', 'Worries', 'about', 'the', 'deficit', 'concerns', 'about', 'China', 'do,', 'however,', 'remain']), (0.07294363942752434, ['\"I', 'think', 'the', \"chairman's\", 'taking', 'a', 'much', 'more', 'sanguine', 'view', 'on', 'the', 'current', 'account', 'deficit', 'than', \"he's\", 'taken', 'for', 'some', 'time,\"', 'said', 'Robert', 'Sinche,', 'head', 'of', 'currency', 'strategy', 'at', 'Bank', 'of', 'America', 'in', 'New', 'York']), (0.0578277249192793, ['In', 'the', 'meantime,', 'the', 'US', 'Federal', \"Reserve's\", 'decision', 'on', '2', 'February', 'to', 'boost', 'interest', 'rates', 'by', 'a', 'quarter', 'of', 'a', 'point', '-', 'the', 'sixth', 'such', 'move', 'in', 'as', 'many', 'months', '-', 'has', 'opened', 'up', 'a', 'differential', 'with', 'European', 'rates']), (0.05702960534038705, ['But', 'calls', 'for', 'a', 'shift', 'in', \"Beijing's\", 'policy', 'have', 'fallen', 'on', 'deaf', 'ears,', 'despite', 'recent', 'comments', 'in', 'a', 'major', 'Chinese', 'newspaper', 'that', 'the', '\"time', 'is', 'ripe\"', 'for', 'a', 'loosening', 'of', 'the', 'peg']), (0.05305884662345338, ['The', 'G7', 'meeting', 'is', 'thought', 'unlikely', 'to', 'produce', 'any', 'meaningful', 'movement', 'in', 'Chinese', 'policy']), (0.0506383842901596, ['In', 'late', 'trading', 'in', 'New', 'York,', 'the', 'dollar', 'reached', '$1.2871', 'against', 'the', 'euro,', 'from', '$1.2974', 'on', 'Thursday'])]\n",
      "Summarize Text: \n",
      " Dollar gains on Greenspan speech The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise. And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments\n"
     ]
    }
   ],
   "source": [
    "generate_summary(\"002.txt\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
